{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sneeraj7376/Netflix-Movies-and-Tv-Shows-Clustering/blob/main/Netflix_movies_and_Tv_Shows_Clustering_ML_Submission_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Netflix Movies and TV shows Clustering\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this priject is to analyze the Netflix Dataset of movies and Tv show until 2019. sourced from the third party search engine flixable. The goal is to group the content into relevant clusters using NLP techniques to improve the user experience througha recommendation system.This will help prevent subscriber chum for netflix, which currently has 220 million subscribers.\n",
        "\n",
        "The project followed a step by step process:\n",
        "1. Handling null values in the datset.\n",
        "2. Managing nested columns (director,cast,listed_in country) for better visualization.\n",
        "3. Binning the rating atrribute into categories (adults,children's,family-friendly,not raed).\n",
        "4. Performing Exploratory Data Analysis to gain insight for preventing subscribers churn.\n",
        "5. Creating clusters using attributes like director,cast,country,genre,rating and descriptions. These attributes were tokenized, preprocessed, and vectorized using TF-IDF vectorizer.\n",
        "6. Reducing the dimensionlity of the dataset using PCA to improve performance.\n",
        "7. Employing K-means clustering and Agglomerative Hierarchical clustering algorithms, determing optimal cluster numbers (4 for K-means,2 for hierarchical clustering) through various evaluation methods.\n",
        "8. Developing a content-based recommender system using cosine similarity matrix to provide personalized recommendations to users and reduce subscriber chum for Netflix"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Business Context\n",
        "\n",
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine. In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming serviceâ€™s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n",
        "\n",
        "In this project, you are required to do\n",
        "\n",
        "* Exploratory Data Analysis\n",
        "* Understanding what type content is available in different countries\n",
        "* If Netflix has been increasingly focusing on TV rather than movies in recent years.\n",
        "* Clustering similar content by matching text-based features**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import string\n",
        "string.punctuation\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# libraries used to implement clusters\n",
        "from sklearn.metrics import silhouette_score\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from textblob import TextBlob\n",
        "from IPython.display import Image\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.utils import simple_preprocess\n",
        "import gensim\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing datasets.\n",
        "df = pd.read_csv('/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING (1).csv')"
      ],
      "metadata": {
        "id": "2I8_Zta8BJQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f'we have total {df.shape[0]} rows and {df.shape[1]} columns.')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\" The total number of duplicated observation in the dataset: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's good to see that we do not have any duplicated observation in our dataset"
      ],
      "metadata": {
        "id": "GPsucHiT2_v2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"-\"*50)\n",
        "print(\"null value count in each of the variable: \")\n",
        "print(\"-\"*50)\n",
        "print(df.isna().sum())\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Percentage of null values in each category\n",
        "print(\"percentage of null values in each variable: \")\n",
        "print(\"-\"*50)\n",
        "null_count_by_variable = df.isnull().sum()/len(df)\n",
        "print(f\"(null_count_by_variable*100)%\")\n",
        "print(\"-\"*50)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# checking Null Value by plotting Heatmap\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(df.isnull(),cbar=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(15,8))\n",
        "plots=sns.barplot(x=df.columns,y=df.isna().sum())\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "for bar in plots.patches:\n",
        "  plots.annotate(bar.get_height(),\n",
        "                 (bar.get_x() + bar.get_width() / 2,\n",
        "                  bar.get_height()), ha='center', va='center',\n",
        "                 size=12,xytext=(0, 8),\n",
        "                 textcoords='offset points')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "t-zpkYNDHxd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Dataset 'Netflix movies and tv shows clustering comprises 12 columns, with only one column aving an integer data type. it does not contain any duplicate values, but it does have null values in 5 columns: director,cast, country,date_added,and rating.\n",
        "\n",
        "This dataset provides a valuable resorce for exploring trends in the range of movies and TV shows avialable on Netflix. Additionaly,it can be utilized for developing clustering models to categorize similar titles together based on shared attributes such as genre, country of origin,and rating."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(f\"Available columns:\\n{df.columns.to_list()}\")"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all').T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Variable description of the Netflix movies and TV shows clustering dataset is as follows:\n",
        "1. show_id:Unique identifier for each movies/shows.\n",
        "2. type:Indicates wheather the entry is a movie or TV show.\n",
        "3. title:Name of the movie or TV show.\n",
        "4. director:Name of the director(s) of the movie or TV Show.\n",
        "5. cast:Name of the actor and actresses featured in the movie or TV show.\n",
        "6. country:Country or countries were the movie or TV shows was produced.\n",
        "7. data_added:Data when the movie or TV shows was added to Netflix.\n",
        "8. release_year:Year when the movie or TV shows was released.\n",
        "9. rating:tv rating or movie rating of the movie or Tv show.\n",
        "10. duration:Lenght of the movie or tv show in the minutes or seasons.\n",
        "11. listed_in:Categories or genres of the movie or TV shows.\n",
        "12. description:Brief synopsis or summary of the movie or Tv show."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(f\"The number of unique values in: \")\n",
        "print(\"-\"*35)\n",
        "for i in df.columns:\n",
        "  print(f\"'{i}' : {df[i].nunique()}\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Missing Null values from each feature"
      ],
      "metadata": {
        "id": "xjRobLWcRrTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null values Count\n",
        "print(\"-\"*50)\n",
        "print(\"Null Value count in each of the variable: \")\n",
        "print(\"-\"*50)\n",
        "print(df.isna().sum())\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Let's find out the percentage of null values in each category in order to deal with it.\n",
        "print(\"percentage of null values in each variable: \")\n",
        "print(\"-\"*50)\n",
        "null_count_by_variable = df.isnull().sum()/len(df)\n",
        "print(f\"{null_count_by_variable*100}%\")\n",
        "print(\"-\"*50)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"date_added\"].value_counts()"
      ],
      "metadata": {
        "id": "bYolQqSGTnq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['rating'].value_counts()"
      ],
      "metadata": {
        "id": "79hsQdV9T1eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['country'].value_counts()"
      ],
      "metadata": {
        "id": "Gwk7sScbUFCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Since 'date_added' and 'rating' has very less percentage of null count so we can drop those obsevations to avoid any business in our clustering model.\n",
        "2. We cannot drop or impute any values in 'directoe' and 'cast' as the null percentage is comparatevely high and we do not know data of those actual movie/TV shows, so ite better to replace those entries with 'unknown'.\n",
        "3. We can fill null values of 'country' with mode as we only have 6% null values and most of the movies/shows are fromUS only."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputing null value as per our discussion\n",
        "# imputing with unknown in null values of director and cast feature\n",
        "df[['director','cast']]=df[['director','cast']].fillna(\"unknown\")\n",
        "\n",
        "# Imputing null values of country with Mode\n",
        "df['country']=df['country'].fillna(df['country'].mode()[0])\n",
        "df.dropna(axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "n6xNXxggV-in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rechecking the Missing Values/Null Values Count\n",
        "print(\"-\"*50)\n",
        "print(\"Null value count in each of the variable: \")\n",
        "print(\"-\"*50)\n",
        "print(df.isna().sum())\n",
        "print(\"-\"*50)\n",
        "\n",
        "#Rechecking the percentage of null values in each category\n",
        "print(\"Percentage of unll values in each variable: \")\n",
        "print(\"-\"*50)\n",
        "null_count_by_variable = df.isnull().sum()/len(df)\n",
        "print(f\"{null_count_by_variable*100}%\")\n",
        "print(\"-\"*50)"
      ],
      "metadata": {
        "id": "ktDl4HNaXubf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a copy of Dataframe and unnest the original one\n",
        "df_new = df.copy()"
      ],
      "metadata": {
        "id": "xZGGRxFeZCpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unnest 'Directors' column\n",
        "dir_constraint=df['director'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df1 = pd.DataFrame(dir_constraint, index = df['title'])\n",
        "df1 = df1.stack()\n",
        "df1 = pd.DataFrame(df1.reset_index())\n",
        "df1.rename(columns={0:'Directors'},inplace=True)\n",
        "df1 = df1.drop(['level_1'],axis=1)\n",
        "df1.sample(10)"
      ],
      "metadata": {
        "id": "88_goeiDZVrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unnested cast column\n",
        "cast_constraint=df['cast'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df2 = pd.DataFrame(cast_constraint,index = df['title'])\n",
        "df2 = df2.stack()\n",
        "df2 = pd.DataFrame(df2.reset_index())\n",
        "df2.rename(columns={0:'Actors'},inplace=True)\n",
        "df2 = df2.drop(['level_1'],axis=1)\n",
        "df2.sample(10)"
      ],
      "metadata": {
        "id": "rnLsh8hsfwJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unnesting 'listed_in' column\n",
        "cast_constraint=df['listed_in'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df3 = pd.DataFrame(cast_constraint, index = df['title'])\n",
        "df3 = df3.stack()\n",
        "df3 = pd.DataFrame(df3.reset_index())\n",
        "df3.rename(columns={0:'Genre'},inplace=True)\n",
        "df3 = df3.drop(['level_1'],axis=1)\n",
        "df3.sample(10)"
      ],
      "metadata": {
        "id": "lARVrzJ9g-FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unnesting 'country' column\n",
        "cast_constraint=df['country'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df4 = pd.DataFrame(cast_constraint,index = df['title'])\n",
        "df4 = df4.stack()\n",
        "df4 = pd.DataFrame(df4.reset_index())\n",
        "df4.rename(columns={0:'Country'},inplace=True)\n",
        "df4 = df4.drop(['level_1'],axis=1)\n",
        "df4.sample(10)"
      ],
      "metadata": {
        "id": "Gpr23yvQn0sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, we have sucessfully separated the nested columns.Now let's just merge all the created dataframe into the single merged dataframe."
      ],
      "metadata": {
        "id": "e6X3pmGqolur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging all the unnested dataframes\n",
        "# Merging director and cast\n",
        "df5 = df2.merge(df1,on=['title'],how='inner')\n",
        "\n",
        "# Merging listed_in with merged of (director and cast)\n",
        "df6 = df5.merge(df3,on=['title'],how='inner')\n",
        "\n",
        "# Merging country with merged of [listed_in with merged of(director and cast)]\n",
        "df7 = df6.merge(df4,on=['title'],how='inner')\n",
        "\n",
        "# head of final merged dataframe\n",
        "df7.head()"
      ],
      "metadata": {
        "id": "Ld0QkWyDo7hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, now let's merge this dataframe with the original one on the left join to avoid information loss"
      ],
      "metadata": {
        "id": "eJk1BvprycvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging unnested data with the created dataframe in order to make the final dataframe\n",
        "df = df7.merge(df[['type', 'title', 'date_added', 'release_year', 'rating', 'duration', 'description']],on=['title'],how='left')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ohXrdinyyunb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Typecasting of attributes"
      ],
      "metadata": {
        "id": "0uDJKHl0zveN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking info of the dataset before typecasting\n",
        "df.info()"
      ],
      "metadata": {
        "id": "_xkyA2nnzu30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tpyecasting duration into integer by removing 'min' and 'season' from the end\n",
        "df['duration'] = df['duration'].apply(lambda x: int(x.split()[0]))\n",
        "\n",
        "# Typecasting string object to datatime object of date_added column\n",
        "df['date_added']= pd.to_datetime(df['date_added'])\n",
        "\n",
        "# Extracting date, day, month and year from date_added column\n",
        "df[\"day_added\"]= df[\"date_added\"].dt.day\n",
        "df[\"month_added\"]= df[\"date_added\"].dt.month\n",
        "df[\"year_added\"]= df[\"date_added\"].dt.year\n",
        "\n",
        "# Dropping date_added\n",
        "df.drop('date_added', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "tgWOsBpe0Jlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking info of the dataset after typecasting\n",
        "df.info()"
      ],
      "metadata": {
        "id": "85xNVtr71k-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binning of Rating attribute"
      ],
      "metadata": {
        "id": "4WuGF7Vm15ut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In rating columns we have different categories these are content rating classifications that are commonly used in the united states and other countries to indicate the appropriateness of media content for different age group.Let's understand each of them and binning them accordingly:\n",
        "\n",
        "TV-MA: This rating is used for mature audiences only, and it may contain strong language violence,nudity and sexual content.\n",
        "\n",
        "R: This rating is used for movies thar are intended for audiences 17 and older.It may contain graphic violence,strong language,drug use and sexual content.\n",
        "\n",
        "PG-13: This rating is used for movies that may not be suitable for children under 13.It may contain violence,mild to moderate language and suggestive content\n",
        "\n",
        "TV-14: This rating is used for TV shows that may not be suitable for children under 14. It may contain violence,strong language,sexual situation, and suggestive dialogue.\n",
        "\n",
        "TV-PG: This rating is used for TV shows that may not be suitable for children under 8.It may contain mild violence,language and suggestive content.\n",
        "\n",
        "NR: This stands for \"NOT Rated.\" It means that the content has not been rated by a rating board,and it may containmaterial that is not suitable for all audiences.\n",
        "\n",
        "TV-G:This rating is used for TV shows that are suitable for all ages.It may contain some mild violence,language and suggestive content.\n",
        "\n",
        "TV-Y: This rating is used for children TV shows that are suitable for all ages.It is intended to be appropriate for preschool children.\n",
        "\n",
        "TV-Y7: This rating is used for children TV shows that may not be suitable for children under 7.It may contain mild violence and scary content.\n",
        "\n",
        "PG: This rating is used for movies that may not br suitable for children under 10.It may contain mild language,some violence and some suggestive content.\n",
        "\n",
        "G: This rating used for movies that are suitable for general audiences.It may contain mild language and some violence.\n",
        "\n",
        "NC-17: This rating is used for movies that are intended for adults only.It may contain explicit sexual content,violence and language.\n",
        "\n",
        "TV-Y7-FV: This rating is used for children's TV shows that may not be suitable for children under 7. It may contain fantasy violence.\n",
        "\n",
        "UR: This stands for \"Unrated\". It means that the content has not been rated by a rating board,and it maycontain material that is not suitable for all audiences."
      ],
      "metadata": {
        "id": "X18J4F2u1_kO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's not complicate it and create bins as following:\n",
        "\n",
        "*   Adult Content: TV-MA, NC-17,R\n",
        "*   Children Content: TV-PG,PG,TV-G,G\n",
        "*   Teen Content: PG-13,TV-14\n",
        "*   Family-friendly Content: TV-y,TV-Y7,TV-Y7-FV\n",
        "*   Not Rated: NR,UR\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7igmBbcAHGDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning the values in the rating column\n",
        "rating_map = {'TV-MA':'Adult Content',\n",
        "              'R':'Adult Content',\n",
        "              'PG-13':'Teen Content',\n",
        "              'TV-14':'Teen Content',\n",
        "              'TV-PG':'Children Content',\n",
        "              'NR':'Not Rated',\n",
        "              'TV-G':'Children Content',\n",
        "              'TV-Y':'Family-friendly Content',\n",
        "              'TV-Y7':'Family-friendly Content',\n",
        "              'PG':'Children Content',\n",
        "              'G':'Children Content',\n",
        "              'NC-17':'Adult Content',\n",
        "              'TV-Y7-FV':'Family-Friendly Content',\n",
        "              'UR':'Not Rated'}\n",
        "df['rating'].replace(rating_map,inplace = True )\n",
        "df['rating'].unique()"
      ],
      "metadata": {
        "id": "iWPEvbNRItfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking head after binning\n",
        "df.head()"
      ],
      "metadata": {
        "id": "UdWnuL6ELA2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separating Movies and TV Shows"
      ],
      "metadata": {
        "id": "t7_U3zs3LVr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating the dataframes for further analysis\n",
        "df_movies=df[df['type']== 'Movie']\n",
        "df_tvshows=df[df['type']=='TV Show']\n",
        "\n",
        "# Printing the shape\n",
        "print(df_movies.shape, df_tvshows.shape)"
      ],
      "metadata": {
        "id": "wUGMka86LVVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "F-uLCzFMMScx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have divided data warngling into five different sections.\n",
        "\n",
        "1. In this section we have imputed/drop the null values of:\n",
        "   * imputed'director' and 'cast' with 'unknown'.\n",
        "   * imputed 'country' with Mode.\n",
        "   * Drop null values of 'date_added' and 'rating'(less percentage).\n",
        "2. We have innested values from following features:\n",
        "   * 'director'\n",
        "   * 'cast'\n",
        "   * 'listed_in'\n",
        "   * 'country'\n",
        "   we have unnested the values and stored in different dataframes and merged all the dataframe with the original one using left join in order to get the isolated values of each of the feature.\n",
        "3. We have typecasted the following features:\n",
        "   * 'duration' into integer(Removing min and seasons from the values).\n",
        "   * 'date_added' to datetime(into the required format)\n",
        "* We have also extracted the following features:   \n",
        "   * 'date' from 'date_added'\n",
        "   * 'month' from 'date_added'\n",
        "   * 'year' from 'date_added'\n",
        "4. We have that the 'rating' column contains various coded categories,so we have decided to create 5 bins and distribute the values accordingly:\n",
        "   * Adult: TV-MA,NC-17\n",
        "   * Restricted: R,UR\n",
        "   * Teen: PG-13,TV-14\n",
        "   * All Ages: TV-G,TV-Y,TV-Y7,TV-Y7-FV,PG,G,TV-PG\n",
        "   * Not Rated: NR\n",
        "5. Lastly we have splitted the dataframe into two df one is 'df_movies' that contains only Movies and the other is 'df_tvshows'that contains only TV Shows for our further analysis.                "
      ],
      "metadata": {
        "id": "Dc7DDtcnM16e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "labels = ['TV Show', 'Movie']\n",
        "values = [df.type.value_counts()[1],df.type.value_counts()[0]]\n",
        "\n",
        "# Colors\n",
        "colors = ['#ffd700', '#008000']\n",
        "\n",
        "#create pie chart\n",
        "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.6)])\n",
        "\n",
        "# Customize layout\n",
        "fig.update_layout(\n",
        "    title_text='Type of Content Watched on Netflix',\n",
        "    title_x=0.5,\n",
        "    height=500,\n",
        "    width=500,\n",
        "    legend=dict(x=0.9),\n",
        "    annotations=[dict(text='Type of Content', font_size=20, showarrow=False)]\n",
        ")\n",
        "\n",
        "# set colors\n",
        "fig.update_traces(marker=dict(colors=colors))"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph shows us the percent of TV shows and movie data present on Netflix Data set"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We can see that majority of the  content on Netflix is movies,which account for around two-thirds of the total content.TV shows make up the remaining one-third of the content.\n",
        "2. We can conclude that in the given data set only 28.3% are TV shows and 71.7% are movies"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. yes! the production house should more focus on quality movies because there is high competition in the market.\n",
        "2. TV shows are less in numbers hence good opportunity for business."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(25, 10))\n",
        "\n",
        "for i, j, k in ((df, 'Overall', 0), (df_movies, 'Movie', 1), (df_tvshows, 'TV Show', 2)):\n",
        "    plt.subplot(1, 3, k + 1)\n",
        "    count = i['rating'].value_counts()\n",
        "\n",
        "    # Generate explode list dynamically\n",
        "    explode = [0.1] * len(count.index)\n",
        "\n",
        "    plt.pie(count, labels=count.index, explode=explode,\n",
        "            colors=['orangered', 'dodgerblue', 'lightgreen', 'mediumslateblue', 'yellow'],\n",
        "            autopct='%1.1f%%', labeldistance=1.1, wedgeprops={\"edgecolor\": \"black\", 'linewidth': 1, 'antialiased': True})\n",
        "    plt.title(f\"Distribution of Content Rating on Netflix '{j}'\")\n",
        "    plt.axis('equal')\n",
        "\n",
        "# Move plt.show() outside of the loop\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mFNdvvV44snO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chosen this chart to know the percentage of type of content present in the Netflix"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We found that most of the content present in the Netflix belongs to Adult and the teen categories\n",
        "2. Another important insight we can that Family friendly content less in Movies compared to TV shows."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. For high gains production house should more focus on teen and Adult content\n",
        "2. There is good chances of growth in family-friendly category in TV Shows."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('default')\n",
        "plt.figure(figsize=(23, 8))\n",
        "\n",
        "for i, j, k in ((df_movies, 'Movies', 0), (df_tvshows, 'TV Shows',1)):\n",
        "    plt.subplot(1, 2, k + 1)\n",
        "\n",
        "    # Fix typo in variable name (should be df_movies instead of df_actor)\n",
        "    df_actor = i.groupby(['Actors']).agg({'title': 'nunique'}).reset_index().sort_values(by=['title'], ascending=False)[1:10]\n",
        "\n",
        "    # Fix typo in parameter name (should be data instead of date)\n",
        "    plots = sns.barplot(y=\"Actors\", x='title', data=df_actor, palette='Set1')\n",
        "\n",
        "    plt.title(f'Actors appeared in most of the {j}')\n",
        "    plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "    # Fix typo in the method name (should be bar_label instead of plots.bar_label)\n",
        "    plots.bar_label(plots.containers[0])\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Know which actors are more popular on Netflix"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We found an interesting insight that most of the Actors in movies are from India.\n",
        "2. No popular actors from india in TV Shows."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indian are movie lover, the love to watch movies hence business should target indian audience for movies."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('default')\n",
        "plt.figure(figsize=(23, 8))\n",
        "\n",
        "for i, j, k in ((df_movies, 'Movies', 0), (df_tvshows, 'TV Shows',1)):\n",
        "    plt.subplot(1, 2, k + 1)\n",
        "\n",
        "    # Fix typo in variable name (should be df_movies instead of df_actor)\n",
        "    df_director= i.groupby(['Directors']).agg({'title': 'nunique'}).reset_index().sort_values(by=['title'], ascending=False)[1:10]\n",
        "\n",
        "    # Fix typo in parameter name (should be data instead of date)\n",
        "    plots = sns.barplot(y=\"Directors\", x='title', data=df_director, palette='Paired')\n",
        "\n",
        "    plt.title(f'Directors appeared in most of the {j}')\n",
        "    plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "    # Fix typo in the method name (should be bar_label instead of plots.bar_label)\n",
        "    plots.bar_label(plots.containers[0])\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know which director is popular in movies and which one is popular in TV shows."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We found that most of the movies directed by jen suter.\n",
        "2. Most TV shows directed by ken burns."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movie/TV shows producers can select the popular director for their upcoming projects."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "df_country = df.groupby(['Country']).agg({'title': 'nunique'}).reset_index().sort_values(by=['title'], ascending=False)[1:10]\n",
        "plt.figure(figsize=(16,6))\n",
        "\n",
        "plots = sns.barplot(y=\"Country\", x='title', data=df_country)\n",
        "plt.xticks(rotation = 60)\n",
        "plt.title('Top 10 Countries for content creation')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "    # Fix typo in the method name (should be bar_label instead of plots.bar_label)\n",
        "plots.bar_label(plots.containers[0])\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know which country produces Maximum number of TV shows and movies."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The United state is the top country producing both movies and Tv show on Netflix.This suggests that netflix is heavily influenced by American content\n",
        "2. Indian is the second highest producer of movies on netflix, indicating the growing popularity of bollywood movies worlwide.\n",
        "3. Country like canada,france,japan also have presence in the data set showing diversity of content on the netflix."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the insight gained can have a position impact on Netflix's business by highlighting opportunities for growth and expansion,such as investing in American and Bollywood content and acquiring more diverse content."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(18,5))\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "sns.countplot(x=df['Country'],order=df['Country'].value_counts().index[0:15],hue=df['type'],palette=\"Set1\")\n",
        "plt.xticks(rotation=50)\n",
        "plt.title('Top 15 contries with most content', fontsize=15,fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "for i, j, k in ((df_movies, 'Movies', 0), (df_tvshows, 'TV Shows',1)):\n",
        "    plt.subplot(1, 2, k + 1)\n",
        "\n",
        "    # Fix typo in variable name (should be df_movies instead of df_actor)\n",
        "    df_actor = i.groupby(['Country']).agg({'title': 'nunique'}).reset_index().sort_values(by=['title'], ascending=False)[1:10]\n",
        "\n",
        "    # Fix typo in parameter name (should be data instead of date)\n",
        "    plots = sns.barplot(y=\"Country\", x='title', data=df_actor, palette='Set1')\n",
        "\n",
        "    plt.title(f'Top 10 contries launching {j} back to back')\n",
        "    plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "    # Fix typo in the method name (should be bar_label instead of plots.bar_label)\n",
        "    plots.bar_label(plots.containers[0])\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know which country produces which type of content most."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. India produces most amount of movies in compare to Tv shows.\n",
        "2. Japan and south korea produces more TV shows in compare to movies."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes,the insight gained can have a positive impact on netflix's business by highlighting opportunities for growth and expansion, such as acquiring and producing more movies from india and more TV shows from japan and south korea."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.figure(figsize=(23,8))\n",
        "df_genre = df.groupby(['Genre']).agg({'title': 'nunique'}).reset_index().sort_values(by=['title'], ascending=False)[1:10]\n",
        "plots = sns.barplot(y= \"Genre\",x='title', data = df_genre)\n",
        "plt.title(f'Most popular genre on Netflix')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plots.bar_label(plots.containers[0])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(23, 8))\n",
        "for i, j, k in ((df_movies, 'Movies', 0), (df_tvshows, 'TV Shows',1)):\n",
        "    plt.subplot(1, 2, k + 1)\n",
        "\n",
        "    # Fix typo in variable name (should be df_movies instead of df_actor)\n",
        "    df_actor = i.groupby(['Genre']).agg({'title': 'nunique'}).reset_index().sort_values(by=['title'], ascending=False)[1:10]\n",
        "\n",
        "    # Fix typo in parameter name (should be data instead of date)\n",
        "    plots = sns.barplot(y=\"Genre\", x='title', data=df_actor, palette='Set1')\n",
        "\n",
        "    plt.title(f'Most popular genre of the {j}')\n",
        "    plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "    # Fix typo in the method name (should be bar_label instead of plots.bar_label)\n",
        "    plots.bar_label(plots.containers[0])\n",
        "    plt.yticks(rotation = 45)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph tells us which genre is most popular in Netflix."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. International movies genre is most popular in both the TV show and movies categories.followed by Drama and comedy."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insught gained can have a positive impact on Netflix's business by helping the platform understand what genres and type of content are popular with its audience.This information can help Netflix tailor its content acquistion and production dtrategies to better cater to the preferences of its viewers,which can lead to increased engagement and customer satisfaction."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(20,6))\n",
        "\n",
        "for i, j, k in ((df_movies, 'Movies', 0), (df_tvshows, 'TV Shows',1)):\n",
        "    plt.subplot(1, 2, k + 1)\n",
        "    df_release_year = i.groupby(['release_year']).agg({'title': 'nunique'}).reset_index().sort_values(by=['title'], ascending=False)[1:10]\n",
        "    plots = sns.barplot(x=\"release_year\", y='title', data=df_release_year, palette='husl')\n",
        "\n",
        "    plt.title(f'{j} release_year')\n",
        "    plt.grid(linestyle='--', linewidth=0.3)\n",
        "    for bar in plots.patches:\n",
        "      plots.annotate(bar.get_height(),\n",
        "                     (bar.get_x() + bar.get_width()/2,\n",
        "                      bar.get_height()), ha='center', va='center',\n",
        "                     size=12, xytext=(0,8),\n",
        "                     textcoords='offset points')\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20,6))\n",
        "\n",
        "for i, j, k in ((df_movies, 'Movies', 0), (df_tvshows, 'TV Shows',1)):\n",
        "    plt.subplot(1, 2, k + 1)\n",
        "\n",
        "    # Fix typo in variable name (should be df_movies instead of df_actor)\n",
        "    df_actor = i.groupby(['year_added']).agg({'title': 'nunique'}).reset_index().sort_values(by=['title'], ascending=False)\n",
        "\n",
        "    # Fix typo in parameter name (should be data instead of date)\n",
        "    plots = sns.barplot(y=\"year_added\", x='title', data=df_actor, palette='husl')\n",
        "\n",
        "    plt.title(f'{j} added to Netflix by year')\n",
        "    plt.ylabel(f\"Number of {j} added on Netflix\")\n",
        "    plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "    for bar in plots.patches:\n",
        "      plots.annotate(bar.get_height(),\n",
        "                     (bar.get_x() + bar.get_width()/2,\n",
        "                      bar.get_height()), ha='center', va='center',\n",
        "                     size=12, xytext=(0,8),\n",
        "                     textcoords='offset points')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph shows us how many movies and TV show released and added in a year on netflix."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the number of movies and TV show added on Netflix has been increasing steadily every year.But since 2018,the number of Movies released on Netflix has been lowered and the number of Tv show released has been significantly increased.In terms of movies and TV shows addition,in 2020 Number of movies added as compared to 2019 were very less and on the other side number of Tv shows were more as compare to 2019."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight that the number of movies has decreased since 2018 while the number of TV shows added has significantly increased could potentially lead to negative growth for Netflix. This could be due to various reasons such as changing consumer oreferences,increased completition from other streaming services,and higher production costs associated with creating movies."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(23,8))\n",
        "\n",
        "for i, j, k in ((df_movies, 'Movies', 0), (df_tvshows, 'TV Shows',1)):\n",
        "    plt.subplot(1, 2, k + 1)\n",
        "    df_month = i.groupby(['month_added']).agg({'title': 'nunique'}).reset_index().sort_values(by=['title'], ascending=False)\n",
        "    plots = sns.barplot(x=\"month_added\", y='title', data=df_month, palette='husl')\n",
        "\n",
        "    plt.title(f'{j} added to Netflix by month')\n",
        "    plt.ylabel(f\"Number of{j} added on Netflix\")\n",
        "    plt.grid(linestyle='--', linewidth=0.3)\n",
        "    for bar in plots.patches:\n",
        "      plots.annotate(bar.get_height(),\n",
        "                     (bar.get_x() + bar.get_width()/2,\n",
        "                      bar.get_height()), ha='center', va='center',\n",
        "                     size=12, xytext=(0,8),\n",
        "                     textcoords='offset points')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have plotted this graph to know in which month the movie/Tv shows added is maximum and in which year minmum."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. we found tha October,November and December  are most popular months for TV shows additions.\n",
        "2. January,October and december are the most popular months foe movie addition.\n",
        "3. Februaryis the least popular month for the movies and TV shows to be added onNetflix"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight gained can help Netflix create a positive business impact by identifying the most popular months for new content additions this can help Netflix plan content released during peak periods, leading to increased user engagement and retention."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code.\n",
        "plt.figure(figsize=(23,8))\n",
        "\n",
        "for i, j, k in ((df_movies, 'Movies', 0), (df_tvshows, 'TV Shows',1)):\n",
        "    plt.subplot(1, 2, k + 1)\n",
        "    df_day = i.groupby(['day_added']).agg({'title': 'nunique'}).reset_index().sort_values(by=['title'], ascending=False)\n",
        "    plots = sns.barplot(x=\"day_added\", y='title', data=df_day, palette='husl')\n",
        "\n",
        "    plt.title(f'{j} added to Netflix by day')\n",
        "    plt.ylabel(f\"Number of{j} added on Netflix\")\n",
        "    plt.grid(linestyle='--', linewidth=0.3)\n",
        "    for bar in plots.patches:\n",
        "      plots.annotate(bar.get_height(),\n",
        "                     (bar.get_x() + bar.get_width()/2,\n",
        "                      bar.get_height()), ha='center', va='bottom',\n",
        "                     size=12, xytext=(0,8),\n",
        "                     textcoords='offset points',rotation=90)"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph shows us the day when most of the movies added in a month."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above bar plots,it can be observed that most of the movies and Tv shows are added at the beginning or middle of the month.It could be because most people tend to have more free time at the beginning of the month after getting paid ,and releasing new content during that time could increase viewership.By releasing new content at the beginning and middle of month,subscribers are more lokely to feel that they are getting value for their money,which could lead to increased retention rates."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes,releasing new content at regular intervals helps to keep users engaged with the platform,as they will have something new to look forward to every few weeks. This can lead to increased viewing hours and user satisfaction,both of which can have positive impact on the business."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code.\n",
        "plt.figure(figsize=(20,8))\n",
        "df_year_month = df.groupby(['year_added','month_added']).agg({'title':'nunique'}).reset_index().sort_values(by=['year_added'],ascending=False)\n",
        "sns.lineplot(x= 'year_added', y='title' ,data = df_year_month,palette= 'hls', hue=df_year_month['month_added'],marker='o')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bivariate graph helps us in knowing which month is dominating in adding movie/Tv shows in a year"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We can see that there is no specific trend is followed,instead of this some consecutive year shows month wise trend.\n",
        "2. From 2008 to 2009 we see movies added in the month of february,and from 2009 to 2011 movies added in the month of february and October.\n",
        "3. After 2015 majority content added in the month of October."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Producers should add there movies in the month when audience is more responsive.\n",
        "2. Although no specific tren is shown but most movies should be uploaded in year end with some discount in the subscription."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization\n",
        "plt.figure(figsize=(20,8))\n",
        "df_year_month = df.groupby(['year_added','day_added']).agg({'title':'nunique'}).reset_index().sort_values(by=['year_added'],ascending=False)\n",
        "sns.lineplot(x= 'year_added', y='title' ,data = df_year_month,palette= 'hls', hue=df_year_month['day_added'],marker='o')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This grapph help us in knowing day is more frequent in movie addition year wise."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Movies from 2008 to 2009 added on 5th day of the month.\n",
        "2. Movies from 2009 to 2010 added on 15th day of the month.\n",
        "3. Most of the movies from 2010 to 2012 added in the month end.\n",
        "4. From 2015 onwords most of the novie are added in the month end or mid month."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently most of the movie are added in 15th day of month or at the last day of month,so before releasing the movie consider this trend also."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "plots = sns.distplot(df_movies['duration'],kde=False, color=['green'])\n",
        "\n",
        "plt.title('Distplot with Normal distribution for Movies',fontweight=\"bold\")\n",
        "for bar in plots.patches:\n",
        "      plots.annotate(bar.get_height(),\n",
        "                     (bar.get_x() + bar.get_width()/2,\n",
        "                      bar.get_height()), ha='center', va='bottom',\n",
        "                     size=12, xytext=(0,8),\n",
        "                     textcoords='offset points',rotation=90)"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(23,8))\n",
        "\n",
        "\n",
        "df_duration = df_tvshows.groupby(['duration']).agg({'title': 'nunique'}).reset_index().sort_values(by=['duration'], ascending=False)\n",
        "plots = sns.barplot(x=\"duration\", y='title', data=df_duration, palette='husl')\n",
        "\n",
        "plt.title(f'Barplot of TV Shows Duration')\n",
        "plt.ylabel(f\"Content count\")\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "for bar in plots.patches:\n",
        "      plots.annotate(bar.get_height(),\n",
        "                     (bar.get_x() + bar.get_width()/2,\n",
        "                      bar.get_height()), ha='center', va='bottom',\n",
        "                     size=12, xytext=(0,8),\n",
        "                     textcoords='offset points',rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2K30ucTQwL1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know the duration distribution for Movies and TV shows on Netflix."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The histogram of the distribution of movie duration in minutes on Netflix shows that the *majority of movies on Netflix have a duration between 80 to 120 minutes.\n",
        "2. The countplot of the distribution of TV show duration in seasons on Netflix shows that the most common duration for TV shows on Netflix is one season,followed by two seasons."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "df['count'] = 1\n",
        "data = df.groupby('Country')[['Country','count']].sum().sort_values(by='count',ascending=False).reset_index()[:10]\n",
        "data = data['Country']\n",
        "df_heatmap = df.loc[df['Country'].isin(data)]\n",
        "df_heatmap = pd.crosstab(df_heatmap['Country'],df_heatmap['rating'], normalize=\"index\").T\n",
        "\n",
        "# plotting the heat map\n",
        "fig, ax=plt.subplots(1,1,figsize=(10,8))\n",
        "\n",
        "# Defining order of representation\n",
        "country_order = ['United States','India','United Kingdom', 'Canada', 'France', 'Japan', 'Spain', 'South Korea','Germany', 'Mexico']\n",
        "rating_order = ['Adult Content', 'Teen Content', 'Children Content','Family-friendly Content', 'Not Rated']\n",
        "\n",
        "# Calling and plotting heatmap\n",
        "sns.heatmap(df_heatmap.loc[rating_order, country_order], cmap=\"jet\", square=True, linewidth=2.5, cbar=False, annot=True, fmt='1.0%',\n",
        "            vmax=.6,vmin=0.05, ax=ax, annot_kws={\"fontsize\": 12})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This graph shows us which ccountries producing which type of content the most."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We found that most of the countries produces content related to Adult and teen.\n",
        "2. Among all the countries INDIA has less content in Adult segment than teen content.\n",
        "3. 85% of content is Adult content from spain.\n",
        "4. Canda produces more content related to children and family-friendly content."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Statement1:\n",
        "* Null Hypothesis: There is no significant difference in the proportion rating of drama movies and comedy movies on Netflix.\n",
        "* Alternative Hypothesis: There is a significant in the proportion rating of drama movies and comedy movies on Netflix.\n",
        "\n",
        "\n",
        "Hypothetical Statement2:\n",
        "* Null Hypothesis: The average duration of Tv shows added in the year 2020 on Netflix is not significantly different from the average duration of TV shows added in the year 2021.\n",
        "* Alternative Hypothesis: The proportion of TV shows added on Netflix that are produced in the United State is not significantly different from average duration of TV shows added in the year 2021.\n",
        "\n",
        "Hypothetical Statement3:\n",
        "* Null Hypothesis: The proportion of TV shows added on Netflix that are produced in the United States is not significantly different from the proportion of movies added on Netflix that are produced in the United States.\n",
        "* Alternative Hypothesis: The proportion of TV shows added on Netflix that are produced in the United States in significantly different from the proportion of movies added on Netflix that are produced in the United States."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. State your research hypothesis as a null hypothesis and alternate hypothesis.\n",
        "\n",
        "\n",
        "Null Hypothesis: There is no significant difference in the proportion ratings of drama movies and comedy movies on Netflix.\n",
        "\n",
        "Alternative hypothesis: There is a significant difference in the proportion ratings of drama and comedy movies on Netflix.\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from statsmodels.stats.proportion import proportions_ztest #    This function is used to perform z test of proportion.\n",
        "\n",
        "# Subset the data to only include drama and comedy movoes.\n",
        "subset =df[df['Genre'].str.contains('Drama') | df['Genre'].str.contains('Comedies')]\n",
        "\n",
        "# Calculate the proportion of drama and comedy movies\n",
        "drama_prop = len(subset[subset['Genre'].str.contains('Drama')])/ len(subset)\n",
        "comedy_prop = len(subset[subset['Genre'].str.contains('comedies')])/ len(subset)\n",
        "\n",
        "# set up the parameters for the z-test\n",
        "count = [int(drama_prop * len(subset)), int(comedy_prop * len(subset))]\n",
        "nobs = [len(subset), len(subset)]\n",
        "alternative = 'two-sided'\n",
        "\n",
        "# Perform the z=test\n",
        "z_stat,p_value = proportions_ztest(count=count,nobs=nobs, alternative=alternative)\n",
        "print('z-statistic:', z_stat)\n",
        "print('p-value: ', p_value)\n",
        "\n",
        "#  Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# print the results of the z-test\n",
        "if p_value < alpha:\n",
        "  print(f\"Reject the null hypothesis.\")\n",
        "else:\n",
        "  print(f\"Fail to reject the null hypothesis.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We conclude that there is a significant in the proportion ratings of drama movies and comedy movies on Netflix."
      ],
      "metadata": {
        "id": "zgi-JyIMReuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test we have used to obtain the P-Value is the z-test for proportions."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The z-test for proportions was chosen because we are comparing the proportions of two categorical variables in a sample. The null hypothesis and alternative hypothesis are about the difference in proportions,and we want to determine if the observed difference in proportions is statiscally significant or not.The Z-test for proportions is appropriate for this situation because it allows us to compare two proportions and calculate the probability of obseving the difference we seen in our sample if the null hypothesis were true."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis : The average duration of TV shows added in the year 2020 on Netflix is not significantly different from the average duration of TV shows added in the year 2021.\n",
        "\n",
        "Alternative Hypothesis : The average duration of TV show added in the year 2020 on Netflix is significant different from the average duration of TV shows added in the year 2021."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# To test this hypothesis, we perform a two-sample t-test\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Create two-sample t-test\n",
        "tv_2020 = df[(df['type'] == 'TV Show') & (df['release_year'] == 2020)]\n",
        "tv_2021 = df[(df['type'] == 'TV Show') & (df['release_year'] == 2021)]\n",
        "\n",
        "# Perform two-sample t-test\n",
        "t, p = ttest_ind(tv_2020['duration'].astype(int),\n",
        "                 tv_2021['duration'].astype(int), equal_var=False)\n",
        "print('t-value:', t)\n",
        "print('p-value:', p)\n",
        "\n",
        "# print the results\n",
        "if p < 0.05:\n",
        "  print('Reject null hypothesis. \\nthe average duration of TV shows added in the year 2020 on Netflix is significant different from the average of TV shows added in the year 2021')\n",
        "else:\n",
        "  print(' Failed to reject null hypothesis. \\nthe average duration of TV shows added in the year 2020 on Netflix is not significant different from the average duration of TV shows added in the year 2021')\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test used to obtain the p-value is a two sample test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-sample t-test was chosen because we are comparing the means of two different sample to determine whether they are significantly different. Additional,we assume that the two samples have unequal variances since it is unlikely that the duration of TV shows added in 2020 and 2021 would have the exact same variance."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: The proportion of TV shows added on Netflix that are produced in the United states is not significant different from the proportion of movies added on Netflix that are produced in the United States.\n",
        "\n",
        "Alternative Hypothesis: The proportion of TV Shows added on Netflix that are produced in the United States is significantly different from the proportion of movies added on Netflix that are produced in the United States."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from statsmodels.stats.proportion import proportions_ztest #    This function is used to perform z test of proportion.\n",
        "\n",
        "\n",
        "# Calculate the proportion of drama and comedy movies\n",
        "tv_proportion = np.sum(df_tvshows['Country'].str.contains('United States'))/ len(df_tvshows)\n",
        "movie_proportion = np.sum(df_movies['Country'].str.contains('United States'))/ len(df_movies)\n",
        "\n",
        "\n",
        "# set up the parameters for the z-test\n",
        "count = [int(tv_proportion * len(subset)), int(movie_proportion * len(df_movies))]\n",
        "nobs = [len(df_tvshows), len(df_movies)]\n",
        "alternative = 'two-sided'\n",
        "\n",
        "# Perform the z=test\n",
        "z_stat,p_value = proportions_ztest(count=count,nobs=nobs, alternative=alternative)\n",
        "print('z-statistic:', z_stat)\n",
        "print('p-value: ', p_value)\n",
        "\n",
        "#  Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# print the results of the z-test\n",
        "if p_value < alpha:\n",
        "  print(f\"Reject the null hypothesis.\")\n",
        "else:\n",
        "  print(f\"Fail to reject the null hypothesis.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test used to obtain P-value is a two-sample proportion test."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose this specific statistical test because it is appropriate for comparing two proportions,and it helps us to determine whether the difference between the two proportions is due to chance or not."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Since we have already dealed with null value. So it is not needed now.\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's move ahead, as we have already dealed with null/missing values from our dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Storing the continous value feature in a separate list\n",
        "continous_value_feature = [\"release_year\",\"duration\",\"day_added\",\"month_added\",\"year_added\"]\n",
        "\n",
        "# Checking outliers with the help of box plot for continous feature\n",
        "plt.figure(figsize=(16,5))\n",
        "for n,column in enumerate(continous_value_feature):\n",
        "  plt.subplot(1, 5, n+1)\n",
        "  sns.boxplot(df[column])\n",
        "  plt.title(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although we have some of the anomalies in continous feature but we will not treat by considering outliers as some of the Movies/TV Shows has released or added early on Netflix."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we are taking the copied dataframe as the data having more number of observations resulted in ram exhaustion.\n",
        "df.shape, df_new.shape"
      ],
      "metadata": {
        "id": "ulLe4vwfkvfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning of rating in new dataframe\n",
        "df_new['rating'].replace(rating_map, inplace = True)\n",
        "\n",
        "# Checking sample after binning\n",
        "df_new.sample(2)"
      ],
      "metadata": {
        "id": "ha2_65I_lHK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Textual Columns"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new feature content_detail with the help of other textual attribute\n",
        "df_new[\"content_detail\"]= df_new[\"cast\"]+\" \"+df_new[\"director\"]+\" \"+df_new[\"listed_in\"]+\" \"+df_new[\"type\"]+\" \"+df_new[\"rating\"]+\" \"+df_new[\"country\"]+\" \"+df_new[\"description\"]\n",
        "\n",
        "# Checking the manipulation\n",
        "df_new.head(5)"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df_new['content_detail'] = df_new['content_detail'].str.lower()\n",
        "\n",
        "# Checking the manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "def remove_punctuations(text):\n",
        "  '''This function is used to remove the punctuations from the given sentence'''\n",
        "  # importing needed library\n",
        "  import string\n",
        "  # replacing the punctuations with no space,which in effect deletes the punctuation marks.\n",
        "  translator = str.maketrans('','', string.punctuation)\n",
        "  # return the text stripped off punctuation marks\n",
        "  return text.translate(translator)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing punctuations from the content detail\n",
        "df_new['content_detail']= df_new['content_detail'].apply(remove_punctuations)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "_HTvWaxwtWhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "def remove_url_and_numbers(text):\n",
        "    '''This function is used to remove the URL's and numbers from the given sentences'''\n",
        "    # importing needed libraries\n",
        "    import re\n",
        "    import string\n",
        "\n",
        "    # replacing the URL's with no space\n",
        "    url_number_pattern = re.compile(r'https?://\\s+|www\\.\\s+')\n",
        "    text = re.sub(url_number_pattern,'',text)\n",
        "\n",
        "    # Replacingthe digits with one space\n",
        "    text = re.sub('[^a-zA-Z]',' ',text)\n",
        "\n",
        "    # return the text stripped off URL's and numbers\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing URL's & Remove words and digits contain digits\n",
        "df_new['content_detail']= df_new['content_detail'].apply(remove_url_and_numbers)\n",
        "\n",
        "# Checking the obsevation after manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "c9gNfcUDvnA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Create a set of English stop words\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Displaying stopwords\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "def remove_stopwords_and_whitespaces(text):\n",
        "  '''This function is used for removing the stopwords from the given sentences'''\n",
        "  text = [word for word in text.split() if not word in stopwords('english')]\n",
        "\n",
        "  # joining the list of words with space separator\n",
        "  text= \" \".join(text)\n",
        "\n",
        "  # removing whitespace\n",
        "  text = re.sub(r'\\s+', ' ',text)\n",
        "\n",
        "  # return the manipulation string\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URL's & Remove words and idgits contain digits\n",
        "def remove_stopwords_and_whitespaces(text):\n",
        "\n",
        "  df_new['content_detail'] = df_new['content_detail'].apply(remove_stopwords_and_whitespaces)\n",
        "\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "  df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "G6nzNt031pWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new['content_detail'][0]"
      ],
      "metadata": {
        "id": "qGhC8-F46wE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading needed libraries\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenization\n",
        "df_new['content_detail']=df_new['content_detail'].apply(nltk.word_tokenize)\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']\n",
        "\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "# Importing WordNetLemmatizer from nltk modul\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Creating instance for wordnet\n",
        "wordnet = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizing_sentence(text):\n",
        "  '''This function is used for lemmatizing(changing the given word into meaningfulword) the words from the given sentence'''\n",
        "  text = [wordnet.lemmatize(word) for word in text]\n",
        "\n",
        "  # Joining the list of words with space searator\n",
        "  text = \" \".join(text)\n",
        "\n",
        "  # return the manipulation\n",
        "  return text"
      ],
      "metadata": {
        "id": "XV-AuTb9-Dze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading needed libraries\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Rephrasing text by applying defined lemmatizing function\n",
        "df_new['content_detail']=df_new['content_detail'].apply(lemmatizing_sentence)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "sJQAfm7W_Gg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Lemmatization instead of Stemming for our project because:\n",
        "* Lemmatization produces a more accurate base word : Unlike stemming,Which simply removees the suffix from a word,lemmatization looks at the meaning of the word and its context to produce a more accurate base form\n",
        "* Lemmatization can handle different inflections : Lemmatization can handle various inflections of a word ,including plural forms,verb tenses and comparativee forms, making it useful for natural language processing.\n",
        "* Lemmatization produces real words : Lemmatization always produces a real word that can be found in a dictionary,making it easier to interpret th eresults of text analysis.\n",
        "* Lemmatization improves text understanding : By reducing words to their base form,Lemmatization makes it easier to understand the context and meaning of sentences\n",
        "* Lemmatization supports multiple languages : While stemming may only work well for English,Lemmatization is effective for many different languages,making it a more versatile text processing technique."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "# tokenize the text into words before POS Taging\n",
        "df_new['pos_tags']= df_new['content_detail'].apply(nltk.word_tokenize).apply(nltk.pos_tag)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.head(5)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# Importing needed Libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Creating instance\n",
        "tfidfv = TfidfVectorizer(max_features=30000)"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting on TfidfVectorizer\n",
        "x = tfidfv.fit_transform(df_new['content_detail'])\n",
        "\n",
        "# Checking shape of the formed document matrix\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "lI0-EPbmlWLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used TFIDF vectorization in place of BAG OF WORDS because Tfidf vectorization takes into account the importance of each word in a document.TF-IDF also assigns higher values to rare words that are unique to a particular document,making them more important in the representation."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Dimensionality Reduction"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that dimensionality reduction is needed? Explain why?"
      ],
      "metadata": {
        "id": "RjIJZDkancY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In textual data processing, there are 30000 attributes are created in text vectorization and huge amount of columns cannot be dealed with our local machines,so,we will using the Principle Component Analysis techniques to reduce the dimensions of this huge sparse matrix."
      ],
      "metadata": {
        "id": "bxxaGQHOnsWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction\n",
        "# Importing PCA from sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# defining PCA object with desired number of components\n",
        "pca = PCA()\n",
        "\n",
        "# Defining PCA object model\n",
        "pca.fit(x.toarray())\n",
        "\n",
        "# percent of variance captured by each component\n",
        "variance = pca.explained_variance_ratio_\n",
        "print(f\"Explained variance: {variance}\")"
      ],
      "metadata": {
        "id": "QMgDM2RooYkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the percent of variance captured versus the number of components in irder to determine the reduced dimensions\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(1, len(variance)+1), np.cumsum(pca.explained_variance_ratio_))\n",
        "ax.set_xlabel('Number of components')\n",
        "ax.set_ylabel('Percent of Variance Captured')\n",
        "ax.set_title('PCA Analysis')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dxxw6NAnqKgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is clear from the above plot that 7770 principal components can capture the 100 of variance. For our case we will consider only those number of PC's that can capture 95% of variance."
      ],
      "metadata": {
        "id": "CeCXhBhNcRve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we are passing the argument so that we can capture 95% of variance.\n",
        "# Defining instance\n",
        "pca_tuned = PCA(n_components=0.95)\n",
        "\n",
        "# Fitting and transforming the model\n",
        "pca_tuned.fit(x.toarray())\n",
        "x_transformed = pca_tuned.transform(x.toarray())\n",
        "\n",
        "# Checking the shape of transformed matrix\n",
        "x_transformed.shape"
      ],
      "metadata": {
        "id": "yp8kk4Vfc1B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Which dimensionality reduction technique have you used and why?"
      ],
      "metadata": {
        "id": "gxmhLqQooxSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used PCA (principal component Analysis) for  dimensionality reduction.PCA is a widely used technique for reducing the dimensionality of high-dimensional data sets while retaining most of the information in the original data.\n",
        "\n",
        "PCA works by finding the principal components of the data, which are linear combinations of the original features that capture the maximum amount of variation in the data.By projecting the data into these principal components,PCA can reduce the number of dimensions while retaining most of the information in the original data.\n",
        "\n",
        "PCA is a popular choice for dimensionality reduction because it is simple to implement,computationally efficient and widely available in most data analysis software packages.Additionally,PCA has been extensively studies and has a strong theoretical foundation,making it a reliable and well-understood method."
      ],
      "metadata": {
        "id": "t68pm86tpFQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-means clustering is a type of unsupervised machine learning algorithm used for partitioning a dataset into k-clusters based on similarity of data points.The goal of the algorithm is to minimize the sum of squared distances between each data point and its corresponding cluster centroid.It works iteratively by assigning each data point to its nearest centroid and then re-computing the centroid of each cluster based on the new assignments.The algorithm terminates when the cluster assignments no longer change or when a maximum number of iterations is reched."
      ],
      "metadata": {
        "id": "FVBA7ILEruNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's just itterate over a loop of 1 to 16 cluster and try to find the optimal numbers with ELBOW method."
      ],
      "metadata": {
        "id": "O1CJm6EVtCbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "#Determining optimal value of K using KELBOW visualizer\n",
        "# Importing needed library\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "# Instantiate the cluster model and visualizer\n",
        "model = KMeans(random_state=0)\n",
        "visualizer = KElbowVisualizer(model, k=(1,16),locate_elbow=False)\n",
        "\n",
        "# fit the data to the visualizer\n",
        "visualizer.fit(x_transformed)\n",
        "\n",
        "# Finalize and render the figure\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here it seems that the elbow is forming at the   clusters but before blindly it let's plot one more chart that itterates over the same number of clusters and determines the silhoutte score at every point.\n",
        "\n",
        "Okay,but what is Silhouette Score?\n",
        "\n",
        "The silhouette score is a measure of how similar an object is to its own cluster compared to other cluster.It is used to evaluate the quality of clustering,where a higher score indicates that objects and more similar to their own cluster and dissimilar to other clusters.\n",
        "\n",
        "The silhouette score ranges from -1 to 1,where a score of 1 indicates that the object is well-matched to its own cluster,and poorly-matched to neighboring clusters,Conversely.a score of -1 indicates that the object is poorly-matched its own cluster and well-matched to neighboring clusters."
      ],
      "metadata": {
        "id": "EAvEhA1LgzrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine optimal value of K using KElbowVisualizer\n",
        "# importing needed library\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "# Instantiate the clustering model and visualizer\n",
        "visualizer = KElbowVisualizer(model, k=(2,16), metric='silhouette', timings=True,locate_elbow=False)\n",
        "\n",
        "#Fit the data to the visualizer\n",
        "visualizer.fit(x_transformed)\n",
        "\n",
        "# finalize and render the figure\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "0_BLtb7VuIua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing silhouette score for each k\n",
        "# Importing needed libraries\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Defining Range\n",
        "k_range = range(2,7)\n",
        "for k in k_range:\n",
        "  Kmodel = KMeans(n_clusters=k)\n",
        "  labels = Kmodel.fit_predict(x_transformed)\n",
        "  score = silhouette_score(x, labels)\n",
        "  print(\"k=%d, silhouette score=%f\" % (k, score))"
      ],
      "metadata": {
        "id": "VMVlYXc_yj5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plots (Elbow plot and silhouette plot) it is very clear that the silhouette score is comparatively good for 4 number of clusters so we will consider 4 cluster in kmeans analysis\n",
        "\n",
        "Now let's plot and see how our data points look like after assigning to their respective clusters."
      ],
      "metadata": {
        "id": "ZDjGkb7b0leu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the k-means model on a dataset\n",
        "kmeans = KMeans(n_clusters=4, init='k-means++', random_state=0)\n",
        "\n",
        "# predict the labels of cluster\n",
        "plt.figure(figsize=(10,6), dpi=120)\n",
        "label = kmeans.fit_predict(x_transformed)\n",
        "# Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "# Plotting the results\n",
        "for i in unique_labels:\n",
        "  plt.scatter(x_transformed[label == i , 0], x_transformed[label == i , 1], label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mDFc_bfM1Q2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 4 different clusters but unfortunately the above plot is in TWO-DIMENSIONAL Let's plot the figure in 3D using mplot3d library and see if we are getting the separated clusters."
      ],
      "metadata": {
        "id": "L5Ubtz5p3uFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library to visualize cluster in 3D\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Plot the cluster in 3D\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "colors = ['r','g','b','y']\n",
        "for i in range(len(colors)):\n",
        "  ax.scatter(x_transformed[kmeans.labels_ == i, 2], x_transformed[kmeans.labels_ == i, 0], x_transformed[kmeans.labels_ ==i, 1], c=colors[i])\n",
        "\n",
        "# Rotate the plot 30 degrees around the X axis and 45 degrees around the Z axis\n",
        "ax.view_init(elev=20, azim=-120)\n",
        "ax.set_xlabel('x-axis')\n",
        "ax.set_ylabel('y-axis')\n",
        "ax.set_zlabel('z-axis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sAMvPMsH4LT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool,we can easily differentiate the all 4 clusters with naked eye.Now let's assign the 'Conent' in their respective cluster by appending 1 more attribute in the final dataframe."
      ],
      "metadata": {
        "id": "DufIRD7C6dpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cluster values to the dataframe.\n",
        "df_new['kmeans_cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "qos8kwjI69RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting with defining a function that plot a wordcloud for each of the attribute in the given dataframe."
      ],
      "metadata": {
        "id": "OEu2I1ez7sol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans_wordcloud(cluster_number, column_name):\n",
        "  '''function for Building a wordcloud for the movie/shows'''\n",
        "\n",
        "  # Importing libraries\n",
        "  from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "  # Filter the data by the specified cluster number and column name\n",
        "  df_wordcloud = df_new[['kmeans_cluster', column_name]].dropna()\n",
        "  df_wordcloud = df_wordcloud[df_wordcloud['kmeans_cluster'] == cluster_number]\n",
        "  df_wordcloud = df_wordcloud[df_wordcloud[column_name].str.len() > 0]\n",
        "\n",
        "  # Combine all text documents into a single string\n",
        "  text = \" \".join(word for word in df_wordcloud[column_name])\n",
        "\n",
        "  # Create the word cloud\n",
        "  wordcloud = WordCloud(stopwords = set(STOPWORDS), background_color=\"black\").generate(text)\n",
        "\n",
        "  # Convert the word cloud\n",
        "  image_array = wordcloud.to_array()\n",
        "\n",
        "  # Return the numpy array\n",
        "  return image_array"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the above defined function and plotting the wordcloud of each attribute\n",
        "fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(20,15))\n",
        "for i in range(4):\n",
        "  for j, col in enumerate(['description', 'listed_in', 'country', 'title']):\n",
        "    axs[j][i].imshow(kmeans_wordcloud(i, col))\n",
        "    axs[j][i].axis('off')\n",
        "    axs[j][i].set_title(f'cluster {i}, {col}', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MRcjcKiS-pMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 (Hierarchial Clustering)"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical clustering is a type of clustering algorithm used for grouping similar data points together into clusters based on their similarity,by recursively merging or dividing clusters based on a measure of similarity or distance between them.\n",
        "\n",
        "Let's dive into it by plotting a Dendrogram and then we will  determine the optimal number of clusters."
      ],
      "metadata": {
        "id": "1l4uqS6qBKk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing neede libraries\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "\n",
        "#Hierarchical Clustering\n",
        "distance_linkage = linkage(x_transformed, method = 'ward', metric = 'euclidean')\n",
        "plt.figure(figsize=(25,10))\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('All films/TV shows')\n",
        "plt.ylabel('Euclidean Distance')\n",
        "\n",
        "dendrogram(distances_linkage, no_labels = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, but what is Dendrogram and how to determine the optimal value of cluster?\n",
        "* A dendrogram is a tree-like diagram that records the sequences of merges or splits.More the distance of the vertical lines in the dendrogram,more the distance between those clusters.\n",
        "* From the above Dendrogram we can say that optimal value of clusters is 2. But before assigning the values to respective clusters,let's check the silhouette scores using Agglomerative clustering and follow th ebottom up approach to aggregate the datapoints."
      ],
      "metadata": {
        "id": "4uIVuIMKDonO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing silhouette score for each k\n",
        "# Importing needed libraries\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Range selected from dendrogram above\n",
        "k_range = range(2, 10)\n",
        "for k in k_range:\n",
        "  model = AgglomerativeClustering(n_clusters=k)\n",
        "  labels = model.fit_predict(x_transformed)\n",
        "  score = silhouette_score(x, labels)\n",
        "  print(\"k=%d, silhouette score%f\" % (k, score))"
      ],
      "metadata": {
        "id": "tdqNnwpbFCxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above silhouette scores it is clear that the 2 clusters are optimal value,which is also clear from the above Dendrogram that for 2 clusters the euclidean distances are maximum.\n",
        "\n",
        "Let's again plot the chart and obseve the 2 different formed cluster"
      ],
      "metadata": {
        "id": "tvDkiLYeGZyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the k-means model on a dataset\n",
        "Agmodel = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "\n",
        "# Predict the labels of clusters.\n",
        "plt.figure(figsize=(10,6), dpi=120)\n",
        "label = Agmodel.fit_predict(x_transformed)\n",
        "# Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "# Plotting the results\n",
        "for i in unique_labels:\n",
        "  plt.scatter(x_transformed[label == i , 0], x_transformed[label == i , 1], label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T2wYI4ArG_Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again plotting the 3d Dimensional plot to see the clusters clearly."
      ],
      "metadata": {
        "id": "GkBvWSg3QTGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library to visualize cluster in 3D\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# plot the clusters in 3D\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "colors = ['r','g', 'b', 'y']\n",
        "for i in range(len(colors)):\n",
        "  ax.scatter(x_transformed[Agmodel.labels_ == i, 0], x_transformed[Agmodel.labels_ == i, 1], x_transformed[Agmodel.labels_ == i, 2], c=colors[i])\n",
        "ax.set_xlabel('x-axis')\n",
        "ax.set_ylabel('y-axis')\n",
        "ax.set_zlabel('z-axis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1KdguZihQf04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, we can again easily differentiate the all 2 clusters with naked eye.Now let's assign the 'content' in their respective cluster by appending 1 more attribute in the final dataframe."
      ],
      "metadata": {
        "id": "5YKNmDCZSi6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cluster values to the dataframe.\n",
        "df_new['agglomerative_cluster'] = Agmodel.labels_"
      ],
      "metadata": {
        "id": "_etUymHtTFnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score chart."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's just again define a function that plots wordcloud for different attributes using Agglomerative Clustering."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agglomerative_wordcloud(cluster_number, column_name):\n",
        "  '''function for Building a wordcloud for the movie/shows'''\n",
        "\n",
        "  # Importing libraries\n",
        "  from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "  # Filter the data by the specified cluster number and column name\n",
        "  df_wordcloud = df_new[['agglomerative_cluster', column_name]].dropna()\n",
        "  df_wordcloud = df_wordcloud[df_wordcloud['agglomerative_cluster'] == cluster_number]\n",
        "\n",
        "  # Combine all text documents into a single string\n",
        "  text = \" \".join (word for word in df_wordcloud[column_name])\n",
        "\n",
        "  # Create the word cloud\n",
        "  wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"black\").generate(text)\n",
        "\n",
        "  # Return the word cloud object\n",
        "  return wordcloud"
      ],
      "metadata": {
        "id": "VCiyd8A3UjR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the above defined function and plotting the wordcloud of each attribute\n",
        "fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(20, 15))\n",
        "for i in range(2):\n",
        "  for j, col in enumerate(['description', 'listed_in', 'country', 'title']):\n",
        "    axs[j][i].imshow(agglomerative_wordcloud(i, col))\n",
        "    axs[j][i].axis('off')\n",
        "    axs[j][i].set_title(f'Cluster {i}, {col}',fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fypbAYIRWXmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using cosine similarity as it is a measure of similarity between two non-zero vectors in multidimensional space.It measures the cosine of the angle between the two vectors,which ranges from-1.\n",
        "\n",
        "In this project we have used cosine similarity which is used to determine how similar two documents or pieces of textare. We represent the documents as vectors in a high-dimensional Spaces,where each dimension represents a word or term in the corpus.We can then calculate the cosine similarity between the vectors to determine how similar the documents are based on their word usage.\n",
        "\n",
        "We are using cosine similarity over tf-idf because:\n",
        "* Cosine similarity handles high dimensional sparse data better.\n",
        "* Cosine similarity captures the meaning of the text better than tf-idf. For Example,if two items contain similar word but in different orders, cosine similarity would still consider them similar,whiletf-idf may not.This is because tf-idf only considers the frequency of words in a document and not their order or meaning."
      ],
      "metadata": {
        "id": "nlPYSnG5ZdJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing needed libraries\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Create a tf-IDF Vectorizer object and transform the text data\n",
        "tfidf = TfidfVectorizer(stop_words ='english')\n",
        "tfidf_matrix = tfidf.fit_transform(df_new['content_detail'])\n",
        "\n",
        "# Compute cosine similarity matrics\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "def recommend_content(title, cosine_sim=cosine_sim, data=df_new):\n",
        "  # Get the index of the input title in the programme_list\n",
        "  programme_list = data['title'].to_list()\n",
        "  index = programme_list.index(title)\n",
        "\n",
        "  # Create a list of tuples containing the similarity sacore and index\n",
        "  # Between the input title and all other programme in the dataset\n",
        "  sim_scores = list(enumerate(cosine_sim[index]))\n",
        "\n",
        "  # sort the list of tuples by similarity score in descending order\n",
        "  sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:11]\n",
        "\n",
        "  # Get the recommended movie title and their similarity scores\n",
        "  recommend_index = [i[0] for i in sim_scores]\n",
        "  rec_movie = data['title'].iloc[recommend_index]\n",
        "  rec_score = [round(i[1], 4) for i in sim_scores]\n",
        "\n",
        "  # Create a pandas Dataframe to display the recommendedtions\n",
        "  rec_table = pd.DataFrame(list(zip(rec_movie, rec_score)), columns=['Recommendation', 'Similarity_score(0-1)'])\n",
        "\n",
        "\n",
        "  return rec_table"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check how our recommender system is performing."
      ],
      "metadata": {
        "id": "1GijwfjxmKbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing indian movie\n",
        "recommend_content('Kal Ho Naa Ho')"
      ],
      "metadata": {
        "id": "c6kJvz7nmJ2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing non indian movie\n",
        "recommend_content('Zombieland')"
      ],
      "metadata": {
        "id": "jtG_Cbl3nrv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing indian TV Shows\n",
        "recommend_content('Zindagi Gulzar Hai')"
      ],
      "metadata": {
        "id": "OG1G2bc4n29_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing non indian TV Shows\n",
        "recommend_content('Vampires')"
      ],
      "metadata": {
        "id": "-bR0SjVsoEsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have chosen Silhouette score over Distortionscore(also known as intertia or sum of squared distances) as evaluation metrics as it measures how well each data point in a cluster is separated from other cluster.it ranges from -1 to 1 with higher values indicating better cluster separation.A silhouette score close to 1 indicates that the data point is well=matched to its own cluster and poorly matched to neighboring clusters.A score close to 0 indicates that the point is on or very close to the boundary between two clusters.A score close to -1 indicates that the data point is probably assigned to the wrong cluster\n",
        "\n",
        "The advantages of using sihouette score over distortion score are:\n",
        "* Silhouette score takes into account both the cohension (how well data points within a cluster are similar) and separation (how well data points in different clusters are dissimilar) of the clusters, whereas distortion score only consider the compactness of each cluster.\n",
        "* Silhouette score is less sensitive to the shape of the cluster,distortion score tends to favor spherical clusters, and in our case the clusters are not completely spherical.\n",
        "* silhouette score provides more intutive and interpretable results,as it  assigns a score to each data point rather than just a single value for the clustering solution."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have considered K-means as our final model, as we are getting the comparatevely high silhouette score in k-means clustering and the resulted clusters are very well separated from each others as have saw in the 3 dimensions.\n",
        "\n",
        "Also in some of the situations K-means works more accurately then other clustering methods such as:\n",
        "* Speed : K-means is generally faster than hierarchical clustering, especially when dealing with large datasets, since it involves fewer calculations and iterations\n",
        "* Ease of use : K-means is relatively straightforward to implement and interpret, as it requires only a few parameters (such as the number of clusters) and produces a clear partitiong of the data.\n",
        "* scalability : K-means can easily handle datasets with a large number of variables or dimensions, whereas hierarchical clustering becomes computationally expansive as the number of data points and dimensions increase.\n",
        "* Independence of clusters : K- means produces non-overlapping clusters, whereas hierarchical clustering can produce overlapping clusters or clusters that are nested within each other,which may not be ideal for certain applications."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Integrating this dataset with external sources such as IMDB ratings,books clustering,plant based type clustering can lead to numerous intriguing discoveries.\n",
        "* By incorporating additional data a more comprehensive recommender system could be developed,offering enhanced recommendations to users.This system could then be deployed on the web for widespread usage."
      ],
      "metadata": {
        "id": "C7TWYP1zrJHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusions drawn from EDA**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the exploratory data analysis (EDA) of the Netflix movies and TV shows clustering dataset,we have drawn the following conclusions:\n",
        "* Movies make up about two-thirds of Netflix content, with Tv shows comprising the remainiing one third.\n",
        "* Adult and teen categories are prevalent on Netflix, while family-friendly content is more common in TV shows than in movies.\n",
        "* Indian actors dominate Netflix movies, while popular Indian actors are absent from Tv shows.\n",
        "* Jan suter is the most common movie director, and ken burns is the most common TV shows director on Netflix.\n",
        "* The United State is the largest producer of movies and TV shows on Netflix, followed by India, Japan and south Korea have more TV shows than movies, indicating growth potential in that area.\n",
        "*  International movies, drama and comedy are the most popular genres on Netflix.\n",
        "* TV show additions on Netflix have increased since 2018, while movie additions have decreased. in 2020, fewer movies were added compared to 2019,but more TV shows were added.\n",
        "* October, November,and December are popular months for adding TV shows, while January, October and November are popular for adding movies. february sees the least additions.\n",
        "* Movies and TV shows are typically added at the beginning or middle of the month and are popularly added on weekends.\n",
        "* Most movies on Netflix have durations between 80 to 120 minutes, while TV shows commonly have one or two seasons.\n",
        "* Various countries contribute adult and teen content, with Spain producing the most adult content and canada focusing on children and family-friendly categories."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusions drawn from ML Model**"
      ],
      "metadata": {
        "id": "0iMBwhxiwsRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Implemented K-means Clustering and Agglomerative Hierarchical Clustering, to cluster the Netflix Movies TV show dataset.\n",
        "* The optimal number of clusters we are getting from K-means is 4, whereas for Agglomerative Hirarchical Clustering the optimal number of cluster are found out to be 2.\n",
        "* We chose Silhouette Score as the evaluation metric over distortion score because it provides a more intuitive and interpretable result. Also silhouette score less sensitive to the shape of the clusters.\n",
        "* Built a Recommendation system that can help Netflix improve user experience and reduce subscriber churn by providing personalized recommendations to users based on their similarty scores."
      ],
      "metadata": {
        "id": "XVD-kZ84w2Zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}